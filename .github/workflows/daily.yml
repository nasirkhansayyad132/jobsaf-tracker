name: Daily Jobs.af Scrape & Notify

on:
  schedule:
    # Run daily at 9:00 AM Kabul time (4:30 AM UTC)
    - cron: '30 4 * * *'
  workflow_dispatch:
    # Allow manual trigger from GitHub UI

permissions:
  contents: write

jobs:
  scrape-and-notify:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install Scraper dependencies
        run: |
          cd scraper
          npm install --no-package-lock
          npx puppeteer browsers install chrome

      - name: Create data directories
        run: |
          mkdir -p data docs/data

      - name: Backup previous jobs for comparison (for 'New' jobs detection)
        run: |
          if [ -f docs/data/jobs.json ]; then
            cp docs/data/jobs.json data/last_jobs.json
            echo "Backed up previous jobs.json to last_jobs.json"
          else
            echo "No previous jobs.json found"
          fi

      - name: Run scraper
        run: |
          cd scraper
          # Output directly to docs/data/jobs.json
          node jobsaf_scrape.js --json ../docs/data/jobs.json --csv ../data/jobs.csv --raw-url "https://jobs.af/jobs?search&category=IT%20-%20Hardware&category=IT%20-%20Software&category=IT%20Billing&category=Data%20Security%2FProtection&category=Software%20Development%20and%20Data%20Management&category=Software%20developer&category=Software%20engineering&category=software%20development%20&category=software%20development&category=software%20analysis&category=Database%20Developing&category=Data%20Management&category=Data%20Collection%20&category=Data%20Entry&category=Data%20analysis&category=Data%20Science&category=Computer%20Science&category=Computer%20Operator&category=Telecommunication%20&category=Computing&category=Database%20Development&category=Data%20Management,%20IT,%20Administration,%20GIS,%20Warehouse,%20Network&category=Data%20analysis%20" --max-pages 20
        continue-on-error: false

      - name: Generate Summary
        run: |
          cd scraper
          # usage: node generate_summary.js <jobs_path> <summary_path> <last_jobs_path>
          node generate_summary.js ../docs/data/jobs.json ../docs/data/summary.json ../data/last_jobs.json

      - name: Set up Python for Email
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Send email notification
        env:
          SMTP_HOST: ${{ secrets.SMTP_HOST }}
          SMTP_PORT: ${{ secrets.SMTP_PORT }}
          SMTP_USER: ${{ secrets.SMTP_USER }}
          SMTP_PASS: ${{ secrets.SMTP_PASS }}
          EMAIL_TO: ${{ secrets.EMAIL_TO }}
        run: |
          # The script expects summary.json as input
          python scripts/notify_email.py docs/data/summary.json
        continue-on-error: true
      
      - name: Commit and push changes
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          # Add data files
          git add docs/data/jobs.json docs/data/summary.json data/jobs.csv || true
          
          # Check if there are changes
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            TIMESTAMP=$(date -u +"%Y-%m-%d %H:%M UTC")
            git commit -m "ðŸ“Š Update jobs data: ${TIMESTAMP}"
            git push
            echo "Changes committed and pushed"
          fi
